# From Cookbook:
# https://cookbook.openai.com/examples/evaluation/getting_started_with_openai_evals
#  This config is the core of the spider-sql eval which is an LLM as a judge eval hooked into the oaiveval testbench
#    In general this test bench is great for research at scale, but newer library base eval frameworks are
#      better for application prod LLMs. Frameworks like deepeval take on more of a unit testing paradigm
#      while preserving unique human in the loop aspects of evals/ large data volume needs for LLM testing.
# 
# From practice:
#    Generated test data using this Python script:
#       See script we copied from cookbook here -- evals/elsuite/modelgraded/scripts/dataset/generate_evals.py
#    Output from the that data at evals/registry/data/sql/my_spider_sql.jsonl 
#       This format matches the actual eval data at sql/spider_sql.jsonl
spider-sql:
  id: spider-sql.dev.v0
  metrics: [accuracy]
  description: Eval that scores SQL code from 194 examples in the Spider Text-to-SQL test dataset. The problems are selected by taking the first 10 problems for each database that appears in the test set.
    Yu, Tao, et al. \"Spider; A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task.\" Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2018, https://doi.org/10.18653/v1/d18-1425.
  disclaimer: Problems are solved zero-shot with no prompting other than the schema; performance may improve with training examples, fine tuning, or a different schema format. Evaluation is currently done through model-grading, where SQL code is not actually executed; the model may judge correct SQL to be incorrect, or vice-versa.
spider-sql.dev.v0:
  class: evals.elsuite.modelgraded.classify:ModelBasedClassify
  args:
    # samples_jsonl: sql/spider_sql.jsonl
    #  Using my smaller generated data to test without incurring major costs.
    samples_jsonl: sql/spider_sql.jsonl
    eval_type: cot_classify
    modelgraded_spec: sql